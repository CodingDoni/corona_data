# -*- coding: utf-8 -*-
"""DataAnalysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mkbK87qSD9hymrOvpF8oMMna3fxi9gre

# connect to google drive

Connecting to GoogleDrive is not necessary for Mr. Weiser
"""

from google.colab import drive
drive.mount('/content/drive')

"""# import packages"""

from numpy.random import seed
import pandas as pd
import sklearn
import tensorflow as tf
from sklearn.utils import resample
from math import ceil
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
import numpy as np
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.callbacks import EarlyStopping
from sklearn.model_selection import KFold
from keras.activations import activation_layers
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from statistics import mean
from numpy.random import seed

"""We tried to make the results of the neural network as reproducible as possible by setting seeds. this ensures less variance in the results. However with neural nets colab with keras, you can't make 100% reproducible results."""

# Seed value
# Apparently you may use different seed values at each stage
v_seed = 0

# 1. Set `PYTHONHASHSEED` environment variable at a fixed value
import os
os.environ['PYTHONHASHSEED']=str(v_seed)

# 2. Set `python` built-in pseudo-random generator at a fixed value
import random
random.seed(v_seed)

# 3. Set `numpy` pseudo-random generator at a fixed value
import numpy as np
np.random.seed(v_seed)

# 4. Set the `tensorflow` pseudo-random generator at a fixed value
import tensorflow as tf
tf.random.set_seed(v_seed)

#from keras import backend as K
#session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)
#sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)
#K.set_session(sess)

"""# read in data

If you want to load the data set, please click on the folder icon on the left and then on the icon with the file and the arrow up. There you have to select the data set "full_data.csv" and load it into the session memory. The variable weiser_mode must be = True.
"""

weiser_modus = False

if weiser_modus:
  data_path = "/content/full_data.csv"
else:
  data_path = "/content/drive/MyDrive/DataAnalysis/full_data.csv"
  
data = pd.read_csv(data_path)

"""# explore data"""

# how does the dataframe look like?
print(data.head())

"""# Preprocessing

We want to get additional information about whether a country is in winter or summer at any given time. To do this, we need to find out whether all countries in the data set are in the northern or southern hemisphere.
"""

# keep track of which country is on which hemisphere
northern_hemisphere = [
    'Afghanistan','Algeria','Bangladesh','Burkina Faso','Cameroon','Canada','China',
    'Cote d\'Ivoire','Egypt','Ethiopia','France','Germany','Ghana','India','Indonesia',
    'Iran','Iraq','Italy','Japan','Kenya','Malaysia','Mali','Mexico','Morocco','Myanmar',
    'Nepal','Niger','Nigeria','North Korea','Pakistan','Philippines','Poland','Romania',
    'Russia','Saudi Arabia','South Korea','Spain','Sri Lanka','Sudan','Syria','Taiwan',
    'Thailand','Turkey','Ukraine','United Kingdom','United States','Uzbekistan','Venezuela',
    'Vietnam','Yemen',"Cayman Islands","Saint Kitts and Nevis","Costa Rica","Serbia","Singapore",
    "Albania", "Ireland", "Iceland","Hungary",'Hong Kong','Honduras','Haiti','Guyana',"Guinea-Bissau",
    'Guinea','Guernsey','Guatemala','Guam','Grenada',"Greenland","Greece","Gibraltar","Georgia","Gambia",
    "Finland", 'Faeroe Islands','Estonia','Eritrea','El Salvador','Dominica','Djibouti','Cyprus','Czechia',
    'Denmark','Curacao','Azerbaijan','Belarus','Belgium','Finland','Armenia','Austria','Croatia','Cuba',
    'Congo','Chad','Central African Republic','Cape Verde','Cambodia','Bulgaria','Brunei',
    'British Virgin Islands','Bosnia and Herzegovina','Bonaire Sint Eustatius and Saba','Bhutan',
    'Bermuda','Benin','Belize','Barbados','Bahrain','Bahamas','Aruba','Antigua and Barbuda',
    'Anguilla','Andorra','United States Virgin Islands','United Arab Emirates',
    'Turks and Caicos Islands','Tunisia','Turkmenistan','Trinidad and Tobago',
    'Togo','Portugal', 'Norway', 'Slovakia', 'Slovenia', 'Switzerland', 'Sweden',
    'Tunisia','Malta', 'Latvia', 'Liechtenstein', 'Lithuania', 'Netherlands', 'Panama',
    'North Macedonia','Tajikistan', 'Vatican', 'Kazakhstan', 'Kyrgyzstan', 'Lebanon',
    'Luxembourg', 'Liberia', 'Libya','Macao', 'Monaco', 'Montenegro', 'Mauritania',
    'Marshall Islands', 'Micronesia (country)', 'Moldova', 'Mongolia','Montserrat',
    'Nicaragua', 'Northern Mariana Islands', 'Oman', 'Palestine', 'Puerto Rico',
    'Qatar','Saint Lucia', 'Saint Pierre and Miquelon', 'Saint Vincent and the Grenadines',
    'Senegal', 'San Marino','Sierra Leone', 'Suriname', 'South Sudan','Sint Maarten (Dutch part)',
    "Malta","Jordan",'Laos','Kuwait','Jordan','Jersey','Jamaica','Israel','Isle of Man',"Palau","Western Sahara"
 ]

southern_hemisphere = [
    "Angola", "Argentina","Australia","Bolivia",'Dominican Republic','Cook Islands','Botswana',
    'Bolivia','New Caledonia', 'Pitcairn', 'Saint Helena', 'Timor','Tokelau','Wallis and Futuna',
    "Botswana","Brazil","Burundi","Chile","Colombia","Comoros","Democratic Republic of Congo", 
    "East Timor", "Ecuador", "Equatorial Guinea"	,"Eswatini", "Fiji","Gabon", "Indonesia",
    "Kenya", "Kiribati",'Falkland Islands',"Lesotho", "Madagascar",		"Malawi", "Maldives",	
    "Mauritius", "Mozambique","Namibia","Nauru","New Zealand","Papua New Guinea" ,"Paraguay",
    "Peru","Republic of the Congo","Rwanda","Samoa","Sao Tome and Principe","Seychelles",
    "Solomon Islands" ,"Somalia","Tanzania","Tonga","Tuvalu","Uganda","Uruguay","Vanuatu",
    "South Africa","Zambia","Zimbabwe","Niue",'French Polynesia']

"""Some helper functions to create the season column"""

# set helper functions
def set_hemisphere (country):
    if country in northern_hemisphere:
      return 'northern'
    elif country in southern_hemisphere:
      return "southern"
    else:
      # if country is NOT YET in southern or northern hemisphere print it out to append it
      print(country)
      return country
      

def set_season(hemisphere, date):
  season = ""
  if hemisphere == "northern":
    if date == "2021-01":
      #season = "winter"
      season = 0
    elif date == "2021-07":
      #season = "summer"
      season = 1
  elif hemisphere == "southern":
    if date == "2021-01":
      #season = "summer"
      season = 1
    elif date == "2021-07":
      #season = "winter"
      season = 0
  else:
    print("unknown hemisphere")
  return season

def make_binary(median, value):
  new_value = -1
  if median > value:
    new_value = 1
  else:
    new_value = 0
  return new_value

"""Let's initialize a few variables that will store information about the relevant columns"""

TARGET_COLUMN = "new_cases_smoothed_per_million"
TARGET = "low_cases"
COLUMNS_TO_KEEP = [
    TARGET,
    "median_age",
    "population_density", 
    "aged_65_older", 
    "gdp_per_capita", 
    "life_expectancy", 
    "human_development_index",
    "is_summer"
    ]

"""Now we process the data set <br>
1. Find entries that have "OWID" in the iso_code (these are not countries but aggregations like "africa" or similar. 
"""

# Filter out rows where iso code contains "OWID". These are aggregations like "lower income" or "africa" and do not represent specific countries
data = data[~data['iso_code'].str.contains("OWID")]

"""2. Let's filter out all dates that are not January or July 2021 and calulate the mean for every column we need, since the dataset provides daily data."""

import re

# regex for getting all july and january 2021 data
pattern_jul='^2021-07-\d{2}$'
pattern_jan='^2021-01-\d{2}$'

column_we_need_for_now = ["location","date","new_cases_smoothed_per_million","new_cases_per_million",'population_density', 'median_age', 'aged_65_older','gdp_per_capita',"human_development_index","life_expectancy"]

# throw away all column we dont need
data = data[column_we_need_for_now]

# keep all rows that match 2021-01
data_jan = data[data['date'].str.match(pattern_jan)]
# calculate mean for every column, grouping by the specific country
data_jan = data.groupby('location').mean()
# now we have one row for each country for 2021-01
# overwrite the date column, since we dont have day anymore
data_jan["date"] = "2021-01"
# set the location column
data_jan["location"] = data_jan.index


# just do excaclty the same for the july
data_jul = data[data['date'].str.match(pattern_jul)]
data_jul = data_jul.groupby('location').mean()
data_jul["date"] = "2021-07"
data_jul["location"] = data_jul.index

# merge january and july data together
data = pd.concat([data_jan,data_jul])

print(data.head())

"""3. Add hemisphere and season the the dataset"""

# calculacte season for every row in dataframe
data['hemisphere'] = data.apply(lambda row: set_hemisphere(row.location), axis=1)
# append season column to dataframe
data['is_summer'] = data.apply(lambda row: set_season(row.hemisphere, row.date), axis=1)

"""We predict on the column "new_cases_smoothed_per_million". However, we don't need the floating-point number, but a binary-coded value. This "low_cases" column is initially set to -1 in order to then overwrite it"""

# instaniate new column with -1
data[TARGET] = -1

"""Now we split our dataset into one for January and one for July. Both datasets go through the same steps:
1. Throw out all columns that we dont need.
2. Throw out all rows that are empty.
3. Overwrite the "low_cases" column with 0 if the entry is in the low 40% and 1 if it is in the upper 40%.
4. Delete all remaining lines with -1 at "low_cases" (the middle 20%)
5. Then delete the "new_cases_smoothed_per_million" column
6. Binary code all other columns. If value <= median of column -> 0, if greater than 1
"""

print(f"In Total for both Dates we have: {len(data.index)} observations")
# how many % we want to keep
percentage = 0.4

# so data is now a df with all the information, of both months we need
# generate dataframe for january and july
data_january = data[data['date'].isin(["2021-01"])]
#print(data_january)
data_january = data_january[COLUMNS_TO_KEEP +[TARGET_COLUMN]]
data_january = data_january.dropna()
print(f"In total for january we have: {len(data_january.index)} observations")

# calculate lower and upper percentile
lower_quantile = data_january[TARGET_COLUMN].quantile(percentage)
higher_quantile = data_january[TARGET_COLUMN].quantile(1-percentage)

lower_data_january = data_january.loc[data_january[TARGET_COLUMN] <= lower_quantile]

# set low_cases = 0
lower_data_january[TARGET] = 0
#print(len(lower_data_january.index))
higher_data_january = data_january.loc[data_january[TARGET_COLUMN] >= higher_quantile]

# set high_cases = 1
higher_data_january[TARGET] = 1
#print(len(higher_data_january.index))
data_january = pd.concat([lower_data_january,higher_data_january])

data_january = data_january[data_january['low_cases'].isin([1,0])]
data_january = data_january.drop([TARGET_COLUMN], axis=1)

# now, for each row that is not yet binary, transform a binary transformation
for col in data_january.select_dtypes(include=['float']).columns:
    median = data_january[col].median()
    data_january[col] = (data_january[col] > median).astype(int)


# do the same for july
data_july = data[data['date'].isin(["2021-07"])]
data_july = data_july[COLUMNS_TO_KEEP +[TARGET_COLUMN]]
data_july = data_july.dropna()
print(f"In total for july uary we have: {len(data_july.index)} observations")

lower_quantile = data_july[TARGET_COLUMN].quantile(percentage)
higher_quantile = data_july[TARGET_COLUMN].quantile(1-percentage)
#print(lower_quantile)
lower_data_july = data_july.loc[data_july[TARGET_COLUMN] <= lower_quantile]
lower_data_july[TARGET] = 0
#print(len(lower_data_july.index))
higher_data_july = data_july.loc[data_july[TARGET_COLUMN] >= higher_quantile]
higher_data_july[TARGET] = 1
#print(len(higher_data_july.index))
data_july = pd.concat([lower_data_july,higher_data_july])

data_july = data_july[data_july['low_cases'].isin([1,0])]
data_july.to_excel("model.xlsx")
data_july = data_july.drop([TARGET_COLUMN], axis=1)

for col in data_july.select_dtypes(include=['float']).columns:
    median = data_july[col].median()
    data_july[col] = (data_july[col] > median).astype(int)

print(data_january.head())

"""Here, you can decide if you want to use the january data. If use_january == False, july data will be used"""

use_january = False

from keras.utils.vis_utils import plot_model
from sklearn.metrics import confusion_matrix

if use_january:
  model_data = data_january
  month="jan"
else:
  model_data = data_july
  month="jul"

# drop NA and NAN entries
model_data = model_data.dropna()

# shuffle the rows
model_data = model_data.sample(frac=1, random_state=v_seed)

# reset index
model_data = model_data.reset_index(drop=True)

y_data = model_data[TARGET]
x_data = model_data.drop([TARGET],axis=1)

model_data.to_excel(f"data_{month}.xlsx")

# initialize parameters
num_folds = 5
loss_function="binary_crossentropy"
optimizer="adam"
metrics=["accuracy"]
kfold = KFold(n_splits=num_folds, shuffle=True, random_state=v_seed)
verbosity=0
conf_matricis = []

# K-fold Cross Validation model evaluation
fold_no = 1
batch_size=30
activation_function = "relu"

#batch_size=30
#lr = 0.9
#neurons = 100
#epochs = 30

# to use a seed
import keras.initializers

# train a model for every k fold
for train_index, test_index  in kfold.split(x_data, y_data):
  x_train, x_test = x_data.iloc[train_index], x_data.iloc[test_index]
  y_train, y_test = y_data.iloc[train_index], y_data.iloc[test_index]
  # Define the model architecture
  model = Sequential()
  model.add(Dense(10, input_shape=(len(x_train.columns),), activation=activation_function,kernel_initializer=keras.initializers.glorot_uniform(seed=v_seed)))
  model.add(Dense(1, activation="sigmoid", kernel_initializer=keras.initializers.glorot_uniform(seed=v_seed)) )

  # Compile the model
  model.compile(loss=loss_function,
                optimizer=tf.keras.optimizers.Adam(learning_rate=0.2),
                metrics=metrics)

  # Generate a print
  print('------------------------------------------------------------------------')
  print(f'Training for fold {fold_no} ...')

  # Fit data to model
  history = model.fit(x_train, y_train,
            batch_size=batch_size,
            epochs=30,
            verbose=verbosity,
            #callbacks=[early_stopping]
            )

  # Generate predictions
  y_pred = model.predict(x_test)

  # Convert predictions to binary classes
  y_pred_binary = (y_pred > 0.5).astype(int)

  # Calculate confusion matrix
  conf_matrix = confusion_matrix(y_test, y_pred_binary)
  conf_matricis.append(conf_matrix)
  fold_no+=1

"""for all the confusion matrices, we need to calculate the important metrics and also calculate the mean for them"""

accuracies = []
precisions = []
recalls = []
f1s = []

for cm in conf_matricis:
  print(cm)
  disp = ConfusionMatrixDisplay(confusion_matrix=conf_matricis[0],
                               display_labels="test")
  #disp.plot()
  TP = cm[0][0]
  FN = cm[0][1]
  FP = cm[1][0]
  TN = cm[1][1]

  accuracy = (TP + TN)/(TP+TN+FP+FN)
  precision = TP / (TP + FP)
  recall = TP / (TP + FN)
  f1 = (2 * precision * recall) / (precision + recall)

  accuracies.append(round(accuracy,2))
  precisions.append(round(precision,2))
  recalls.append(round(recall,2))
  f1s.append(round(f1,2))

from statistics import stdev

print(f"accuracies: {accuracies}")
print(f"mean: {round(mean(accuracies),2)} - Std: {round(stdev(accuracies),4)}")
print(f"precision: {precisions}")
print(f"mean: {round(mean(precisions),2)} - Std: {round(stdev(precisions),4)}")
print(f"recall: {recalls}")
print(f"mean: {round(mean(recalls),2)} - Std: {round(stdev(recalls),4)}")
print(f"f1-score: {f1s}")
print(f"mean: {round(mean(f1s),2)} - Std: {round(stdev(f1s),4)}")

conf_matrix_1 = conf_matricis[0]
disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix_1,
                               display_labels="test")
disp.plot()

conf_matrix_2 = conf_matricis[1]
disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix_2,
                               display_labels="test")
disp.plot()

conf_matrix_3 = conf_matricis[2]
disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix_3,
                               display_labels="test")
disp.plot()

conf_matrix_4 = conf_matricis[3]
disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix_4,
                               display_labels="test")
disp.plot()

conf_matrix_5 = conf_matricis[4]
disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix_5,
                               display_labels="test")
disp.plot()

plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)

"""Here you can run the "old" model without binary encoding of the features"""

weiser_modus = False

if weiser_modus:
  data_path = "/content/full_data.csv"
else:
  data_path = "/content/drive/MyDrive/DataAnalysis/full_data.csv"
  
data = pd.read_csv(data_path)

# keep track of which country is on which hemisphere
northern_hemisphere = [
    'Afghanistan','Algeria','Bangladesh','Burkina Faso','Cameroon','Canada','China',
    'Cote d\'Ivoire','Egypt','Ethiopia','France','Germany','Ghana','India','Indonesia',
    'Iran','Iraq','Italy','Japan','Kenya','Malaysia','Mali','Mexico','Morocco','Myanmar',
    'Nepal','Niger','Nigeria','North Korea','Pakistan','Philippines','Poland','Romania',
    'Russia','Saudi Arabia','South Korea','Spain','Sri Lanka','Sudan','Syria','Taiwan',
    'Thailand','Turkey','Ukraine','United Kingdom','United States','Uzbekistan','Venezuela',
    'Vietnam','Yemen',"Cayman Islands","Saint Kitts and Nevis","Costa Rica","Serbia","Singapore",
    "Albania", "Ireland", "Iceland","Hungary",'Hong Kong','Honduras','Haiti','Guyana',"Guinea-Bissau",
    'Guinea','Guernsey','Guatemala','Guam','Grenada',"Greenland","Greece","Gibraltar","Georgia","Gambia",
    "Finland", 'Faeroe Islands','Estonia','Eritrea','El Salvador','Dominica','Djibouti','Cyprus','Czechia',
    'Denmark','Curacao','Azerbaijan','Belarus','Belgium','Finland','Armenia','Austria','Croatia','Cuba',
    'Congo','Chad','Central African Republic','Cape Verde','Cambodia','Bulgaria','Brunei',
    'British Virgin Islands','Bosnia and Herzegovina','Bonaire Sint Eustatius and Saba','Bhutan',
    'Bermuda','Benin','Belize','Barbados','Bahrain','Bahamas','Aruba','Antigua and Barbuda',
    'Anguilla','Andorra','United States Virgin Islands','United Arab Emirates',
    'Turks and Caicos Islands','Tunisia','Turkmenistan','Trinidad and Tobago',
    'Togo','Portugal', 'Norway', 'Slovakia', 'Slovenia', 'Switzerland', 'Sweden',
    'Tunisia','Malta', 'Latvia', 'Liechtenstein', 'Lithuania', 'Netherlands', 'Panama',
    'North Macedonia','Tajikistan', 'Vatican', 'Kazakhstan', 'Kyrgyzstan', 'Lebanon',
    'Luxembourg', 'Liberia', 'Libya','Macao', 'Monaco', 'Montenegro', 'Mauritania',
    'Marshall Islands', 'Micronesia (country)', 'Moldova', 'Mongolia','Montserrat',
    'Nicaragua', 'Northern Mariana Islands', 'Oman', 'Palestine', 'Puerto Rico',
    'Qatar','Saint Lucia', 'Saint Pierre and Miquelon', 'Saint Vincent and the Grenadines',
    'Senegal', 'San Marino','Sierra Leone', 'Suriname', 'South Sudan','Sint Maarten (Dutch part)',
    "Malta","Jordan",'Laos','Kuwait','Jordan','Jersey','Jamaica','Israel','Isle of Man',"Palau","Western Sahara"
 ]

southern_hemisphere = [
    "Angola", "Argentina","Australia","Bolivia",'Dominican Republic','Cook Islands','Botswana',
    'Bolivia','New Caledonia', 'Pitcairn', 'Saint Helena', 'Timor','Tokelau','Wallis and Futuna',
    "Botswana","Brazil","Burundi","Chile","Colombia","Comoros","Democratic Republic of Congo", 
    "East Timor", "Ecuador", "Equatorial Guinea"	,"Eswatini", "Fiji","Gabon", "Indonesia",
    "Kenya", "Kiribati",'Falkland Islands',"Lesotho", "Madagascar",		"Malawi", "Maldives",	
    "Mauritius", "Mozambique","Namibia","Nauru","New Zealand","Papua New Guinea" ,"Paraguay",
    "Peru","Republic of the Congo","Rwanda","Samoa","Sao Tome and Principe","Seychelles",
    "Solomon Islands" ,"Somalia","Tanzania","Tonga","Tuvalu","Uganda","Uruguay","Vanuatu",
    "South Africa","Zambia","Zimbabwe","Niue",'French Polynesia']

# set helper functions
def set_hemisphere (country):
    if country in northern_hemisphere:
      return 'northern'
    elif country in southern_hemisphere:
      return "southern"
    else:
      # if country is NOT YET in southern or northern hemisphere print it out to append it
      print(country)
      return country
      

def set_season(hemisphere, date):
  season = ""
  if hemisphere == "northern":
    if date == "2021-01":
      #season = "winter"
      season = 0
    elif date == "2021-07":
      #season = "summer"
      season = 1
  elif hemisphere == "southern":
    if date == "2021-01":
      #season = "summer"
      season = 1
    elif date == "2021-07":
      #season = "winter"
      season = 0
  else:
    print("unknown hemisphere")
  return season

def make_binary(median, value):
  new_value = -1
  if median > value:
    new_value = 1
  else:
    new_value = 0
  return new_value

TARGET_COLUMN = "new_cases_smoothed_per_million"
TARGET = "low_cases"
COLUMNS_TO_KEEP = [
    TARGET,
    "median_age",
    "population_density", 
    "aged_65_older", 
    "gdp_per_capita", 
    "life_expectancy", 
    "human_development_index",
    "is_summer"
    ]

# Filter out rows where iso code contains "OWID". These are aggregations like "lower income" or "africa" and do not represent specific countries
data = data[~data['iso_code'].str.contains("OWID")]

import re

# regex for getting all july and january 2021 data
pattern_jul='^2021-07-\d{2}$'
pattern_jan='^2021-01-\d{2}$'

column_we_need_for_now = ["location","date","new_cases_smoothed_per_million","new_cases_per_million",'population_density', 'median_age', 'aged_65_older','gdp_per_capita',"human_development_index","life_expectancy"]

# throw away all column we dont need
data = data[column_we_need_for_now]

# keep all rows that match 2021-01
data_jan = data[data['date'].str.match(pattern_jan)]
# calculate mean for every column, grouping by the specific country
data_jan = data.groupby('location').mean()
# now we have one row for each country for 2021-01
# overwrite the date column, since we dont have day anymore
data_jan["date"] = "2021-01"
# set the location column
data_jan["location"] = data_jan.index


# just do excaclty the same for the july
data_jul = data[data['date'].str.match(pattern_jul)]
data_jul = data_jul.groupby('location').mean()
data_jul["date"] = "2021-07"
data_jul["location"] = data_jul.index

# merge january and july data together
data = pd.concat([data_jan,data_jul])

# calculacte season for every row in dataframe
data['hemisphere'] = data.apply(lambda row: set_hemisphere(row.location), axis=1)
# append season column to dataframe
data['is_summer'] = data.apply(lambda row: set_season(row.hemisphere, row.date), axis=1)

# instaniate new column with -1
data[TARGET] = -1

print(f"In Total for both Dates we have: {len(data.index)} observations")
# how many % we want to keep
percentage = 0.4

# so data is now a df with all the information, of both months we need
# generate dataframe for january and july
data_january = data[data['date'].isin(["2021-01"])]
#print(data_january)
data_january = data_january[COLUMNS_TO_KEEP +[TARGET_COLUMN]]
data_january = data_january.dropna()
print(f"In total for january we have: {len(data_january.index)} observations")

# calculate lower and upper percentile
lower_quantile = data_january[TARGET_COLUMN].quantile(percentage)
higher_quantile = data_january[TARGET_COLUMN].quantile(1-percentage)

lower_data_january = data_january.loc[data_january[TARGET_COLUMN] <= lower_quantile]

# set low_cases = 0
lower_data_january[TARGET] = 0
#print(len(lower_data_january.index))
higher_data_january = data_january.loc[data_january[TARGET_COLUMN] >= higher_quantile]

# set high_cases = 1
higher_data_january[TARGET] = 1
#print(len(higher_data_january.index))
data_january = pd.concat([lower_data_january,higher_data_january])

data_january = data_january[data_january['low_cases'].isin([1,0])]
data_january = data_january.drop([TARGET_COLUMN], axis=1)


# do the same for july
data_july = data[data['date'].isin(["2021-07"])]
data_july = data_july[COLUMNS_TO_KEEP +[TARGET_COLUMN]]
data_july = data_july.dropna()
print(f"In total for july uary we have: {len(data_july.index)} observations")

lower_quantile = data_july[TARGET_COLUMN].quantile(percentage)
higher_quantile = data_july[TARGET_COLUMN].quantile(1-percentage)
#print(lower_quantile)
lower_data_july = data_july.loc[data_july[TARGET_COLUMN] <= lower_quantile]
lower_data_july[TARGET] = 0
#print(len(lower_data_july.index))
higher_data_july = data_july.loc[data_july[TARGET_COLUMN] >= higher_quantile]
higher_data_july[TARGET] = 1
#print(len(higher_data_july.index))
data_july = pd.concat([lower_data_july,higher_data_july])

data_july = data_july[data_july['low_cases'].isin([1,0])]
data_july.to_excel("model.xlsx")
data_july = data_july.drop([TARGET_COLUMN], axis=1)

accuracies = []
precisions = []
recalls = []
f1s = []

for cm in conf_matricis:
  print(cm)
  disp = ConfusionMatrixDisplay(confusion_matrix=conf_matricis[0],
                               display_labels="test")
  #disp.plot()
  TP = cm[0][0]
  FN = cm[0][1]
  FP = cm[1][0]
  TN = cm[1][1]

  accuracy = (TP + TN)/(TP+TN+FP+FN)
  precision = TP / (TP + FP)
  recall = TP / (TP + FN)
  f1 = (2 * precision * recall) / (precision + recall)

  accuracies.append(round(accuracy,2))
  precisions.append(round(precision,2))
  recalls.append(round(recall,2))
  f1s.append(round(f1,2))

from statistics import stdev

print(f"accuracies: {accuracies}")
print(f"mean: {round(mean(accuracies),2)} - Std: {round(stdev(accuracies),4)}")
print(f"precision: {precisions}")
print(f"mean: {round(mean(precisions),2)} - Std: {round(stdev(precisions),4)}")
print(f"recall: {recalls}")
print(f"mean: {round(mean(recalls),2)} - Std: {round(stdev(recalls),4)}")
print(f"f1-score: {f1s}")
print(f"mean: {round(mean(f1s),2)} - Std: {round(stdev(f1s),4)}")